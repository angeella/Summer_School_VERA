---
title: "<img src=\"unive_0.jpg\" /> Introduction to R"
author: | 
  | Angela Andreella 
  | Ca' Foscari University of Venice
  | angela.andreella@unive.it
date: '2023-06-27'
output:
  prettydoc::html_pretty:
    theme: tactile
    df_print: paged
    toc: true
    number_sections: true
    highlight: vignette
fontsize: 11pt
geometry: margin = 1in
---

<style type="text/css">
.main-container {
  max-width: 1100px;
  margin-left: auto;
  margin-right: auto;
}
</style>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = T, fig.align = "center", out.width = '80%', warning = F, message = F)
```

This tutorial covers basics of `R`, a gentle reminder. You need to install `R` and `Rstudio`. 

You can find all the material for these tutorials [here!](https://github.com/angeella/summer_school_VERA) Special thanks also to Ilaria Bussoli for sharing part of this material. 

# use...R!

 - **Reproducible analyses**
 
 - Free, **Open-Source**
 
 - Continuously **evolving** and expanding
 
 - Large **community**
 
<center> 
![](Images/download.png){width=20%} <style>
        p {line-width: 62em;}
    </style>![](Images/R-LadiesGlobal.png){ width=20% }
</center> 

`Rstudio` is *simpler* than `R`:

<center>
![](Images/Rstudio.jpg)
</center> 
<br>


1. **SCRIPT**: Here you can write your `.R` and `.rmd` files. In the toolbar `r knitr::include_graphics("Images/toolbar.png")` you can save your script clicking on `r emo::ji("save")`.

2. **ENVIROMENT/HISTORY**: Here you can visualize your loaded **objects** (datasets etc) and see the code that you run.

3. **`R` CONSOLE**: Here your code is executed. You can:

   - directly write your code here and press`enter`,
   
   - write the code in the **SCRIPT** window and run it using `ctrl + enter` or using the run bottom in the `r` script window (top right).

4. 
    - **FILES**: explores the local folders
    
    - **PLOTS**: where the plots come up
    
    - **PACKAGES**: where you can install packages
    
    - **HELP**: where you can see how a function is constructed. Alternatively you can use `?` before the function name, i.e., `?function`.

`r emo::ji("boom")` **Please note**! `R` and `Rstudio` are not the same! `R` is a programming language while `Rstudio` is an IDE (integrated development environment) which makes `R` user-friendly.


# Packages for everyone!

`R` provides several **packages** that contain very useful functions of various kinds. For example with the following commands:
 
:::: {style="display: grid; grid-template-columns: auto auto; grid-column-gap: 5px; place-items: start;"}
::: {}
__R commands__ 

```{r install_package, eval = FALSE}
install.packages("ggplot2") 
```
```{r load_package}
library(ggplot2) 
```

:::
::: {}
__Comments__ 

* `install.packages(x)` -- install the package called `x`.
* `library(x)` -- recall the installed package called `x` in our session.
:::
:::: 

So, we install the package ``ggplot2`` and we recall it in our session. This package permits to create fancy graphical plots.

For each idea that you want to apply in `R`, probably a package exists which will resolve all your problems. If you have the following error:

```{r example_error, eval = F, echo = T}
Error in library(dplyr) : there is no package called dplyr
```

means that you must install the package ``dplyr``. 
`r emo::ji("smile")` 

`r emo::ji("boom")` **Please note**! find errors/bugs on google or [stackoverflow](https://stackoverflow.com)!

<center>
![](Images/stackoverflow.png){ width=30% }
</center>

# Basics

## Arithmetic

- `+` for the **addition**

- `-` for the **subtraction**

- `*` for the **multiplication**

- `/` for the **division**


Some examples:

```{r}
3+5
```

```{r}
3-10
```

```{r}
4*pi
```

```{r}
20^(-1)
```

## Logical Comparison

- `>` stands for **GREATER**
- `<` stands for **LESS** 
- `==` stands for **EQUAL**
- `!=` stands for **NOT EQUAL**
- `>=` stands for **GREATER EQUAL**
- `<=` stands for **LESS EQUAL**
- `&` stands for **AND**
- `|` stands for **OR**

Some examples:

```{r}
4<10
```

```{r}
3>10
```

```{r}
(4<10)&(3>10)
```

```{r}
4==10
```

```{r}
8>=10
```

> Warning: The `=` and `<-` symbols are used to assign a value to an object. Different than `==` (logical operation defining equal).

As in all programming language `TRUE==1` and `FALSE==0`.

## Strings

The string is placed between two `"`, e.g.,

```{r}
"Let use R!"
```

## Objects

We can save some values into an object using the sign `=` or `<-`.

For example running:

```{r}
x <- 8
```

we assign the value $8$ to `x`. If we then call `x`, `R` will returns 8:

```{r}
x
```
- You can also assign a string to an object.
 
- You can make operations between objects:
 
```{r}
y <- 10
x*y
```
 
```{r}
z <- 2/3

x == z
```

The object can be: 

1. **logical**: `TRUE` or `FALSE`.

2. **integer**: a real number without a decimal part. 

3. **numeric**: pi, $0.55$, $1.98$, etc. i.e., a real number.

4. **character**: "hello", "I love stat", i.e., a string.

5. **factor**: "hello", "I love stat", i.e., a string with defined levels.

We can check it using the `class()` function:

```{r}
class(x)
```


## Vectors

We specify vectors using `c()`. Inside the `()` we put our values separated by **commas**. 

For example, running:

```{r}
x <- c(3, 20, 1)
```

creates an object `x` which is a vector having dimension equals $3$ with then three values $3$, $20$ and $1$.

We can make **arithmetic operations** between vectors, e.g.,

```{r}
x/3
```

or using functions:

- **Sum**

```{r}
sum(x)
```

- **Product**

```{r}
prod(x)
```

- **Absolute values**

```{r}
x <- c(-1,2,5)
abs(x)
```

- **Exponential**, i.e., `e^x`

```{r}
exp(x)
```

- **Natural logarithm** 

```{r}
x <- c(1,4,10)
log(x)
```
- **Maximum**

```{r}
max(x)
```

- **Minimum**

```{r}
min(x)
```

- **Range**

```{r}
range(x)
```

- **Mean**

```{r}
mean(x)
```

- **Median**

```{r}
median(x)
```

- **Variance**

```{r}
var(x)
```

- **Standard deviation**

```{r}
sd(x)
```

We can also use some specific **functions** to create vector directly if its values have a specific pattern:

```{r}
1:10 #Here we create a vector with integer values from 1 to 10
```


```{r}
seq(1,10) #Here we create a sequence with integer values from 1 to 10
```

```{r}
seq(1,10,by = 2) #Here we create a sequence with integer values from 1 to 10 with an increment equals 2
```

```{r}
seq(1,10,length.out = 3) #Here we create a sequence taking values in [1,10] having length equals 3.
```

Other useful functions:

- Vector **length**:

```{r}
length(x)
```

- **Repeat** a vector:

```{r}
rep(x,3) #Here we repeat the vector x three times
```

- Using **random values** generated from some distribution:

  1. `rnorm()` Normal distribution
  
  2. `runif()` Uniform distribution
  
  3. `rpois()` Poisson distribution
  
  4. `rgamma()` Gamma distribution
  
  5. `rt()` Gamma distribution
  
For example:

```{r}
rnorm(n = 5, mean = 2, sd = 1.5) #Here we genereted 5 values from a Normal distribution with mean 2 and standard deviation 1.5.
```

For a complete look of the distributions available on `R`, see `help(Distributions)`.

We can also extract a part of the vector using the square brackets `[]`:

1. by **numerical indices**:

```{r}
x[c(1, 3)] #Here we extract the first and third elements
```

2. by **logical indices**:

```{r}
x[c(TRUE, FALSE, TRUE)] #Here we extract the first and third elements
```
Note that the object inside the square brackets is a vector, with length equals the number of elements of the vector that we want to extract if we use **numerical indices**, or with length equals the lengths of the vector if **logical indices** are used. Here, you can use also the `seq()`, `rep()`, `:` functions. Finally, you can create the logical indices using some logical comparison, e.g., 

```{r}
x[x>4] #Here we extract the elements of x having value greater than 4.
```
We can use the square brackets also for **changing** one or more values of the vector:

```{r}
x[1] <- 6 #Here we substitute the first element of the vector with 6.
x
```


## Matrices

Matrices are a **two-dimensional data structure**, where elements are arranged according to rows and columns. We can think the matrix as a collection of vectors.

We can create a matrix using the function `matrix()`:

:::: {style="display: grid; grid-template-columns: 1fr 21fr; grid-column-gap: 2px; place-items: start; padding: 2em 2em 2em 2em; border: 5px solid #f8f8f8;"}

::: {}

```{r, echo=1:15}
X <- matrix(data = c(1:20), 
            nrow = 4, 
            ncol = 5, 
            byrow = TRUE)
```
:::

::: {}

* `data`: vector of values used to fill the matrix.

* `nrow`: the desired number of rows.

* `ncol`: the desired number of columns.

* `byrow`: logical value. If FALSE (the default) the matrix is filled by columns, otherwise the matrix is filled by rows.

:::
::::

We can also fill the matrix with strings.

As in the case of vectors, we can extract a subset of the matrix using the quare brackets `[]`. Here, the indices must be **two-dimensional**: one dimension for extracting the rows, and one dimension for extracting the columns.

```{r}
X[1:2, 4] #Here we extract the first and second rows and the fourth column. So two values
```
Also, here we can use logical indices instead of numerical ones or logical comparisons. If you want to select only one dimension, you must put nothing in the other dimension of the square brackets:

```{r}
X[1:2,] #Here we extract the first and second rows and all the columns. So ten values
```
```{r}
X[,1:2] #Here we extract the first and second columns and all the rows So ten values
```

> The selection operation does not change the initial object. Therefore, it is necessary to save the result of the selection if you want to keep the changes. (This is valid also working with vectors).

Other useful functions:

- Matrix **dimension**:

```{r}
dim(X)
```

- Number of **columns**:

```{r}
ncol(X)
```

- Number of **rows**:

```{r}
nrow(X)
```

- **Combine** two or more matrices by columns (matrices must have the same number of rows):

```{r}
Y <- matrix(data = seq(from = 20, to =60, length.out = 5*4), nrow = 4, ncol = 5, byrow = TRUE)
cbind(X, Y)
```
- **Combine** two or more matrices by rows (matrices must have the same number of columns):

```{r}
rbind(X, Y)
```

- **Transpose**, i.e., $X^\top$:

```{r}
t(X)
```

- **Product** between matrices:

```{r}
t(X) %*% Y
```

- **Inverse** of a matrix, i.e. $Z^{-1}$:

```{r}
Z <- matrix(rnorm(5*5), nrow = 5)
solve(Z)
```

- Extract the **diagonal ** of a matrix:

```{r}
diag(Z)
```

- **Determinant** of matrix:

```{r}
det(Z)
```

> Using `+`, `-`, `*`, `/` between two matrices perform the element-wise operations.

## Array

We can create objects that have more than two dimensions, i.e., an array. We use the `array()` command by indicating the vector of values used to populate the object and the size of each of its dimensions:

:::: {style="display: grid; grid-template-columns: 1fr 21fr; grid-column-gap: 2px; place-items: start; padding: 2em 2em 2em 2em; border: 5px solid #f8f8f8;"}

::: {}

```{r, echo=1:15}
X <- array(data = rnorm(2*3*5), 
           dim = c(2,3,5))
```
:::

::: {}

* `data`: vector of values used to fill the matrix.

* `dim`: the dimensions for the array to be created, that is an integer vector of length one or more giving the maximal indices in each dimension.

:::
::::

All the main functions and selection operations we have seen for arrays and vectors can be performed similarly with arrays. We can check the dimensions of the array using the `dim()` command.

## Dataframe

Dataframes are one of the most commonly used objects in `R` to represent data. These objects are two-dimensional as matrices, but they can contain every types of data (e.g., strings and numbers). We can think about dataframes as normal tables from Excel etc.


:::: {style="display: grid; grid-template-columns: 1fr 21fr; grid-column-gap: 2px; place-items: start; padding: 2em 2em 2em 2em; border: 5px solid #f8f8f8;"}
::: {}
```{r}
x_num <- rnorm(9, mean = 5)
y_string <- rep(c("ciao", "hello", "hola"),3)
db <- data.frame(Numbers = x_num, 
                 Strings = y_string, 
                 row.names = NULL, 
                 stringsAsFactors = TRUE)
```
:::
::: {}
* ... = We must specify each column with its column name.

* row.names = `NULL` or a single integer or character string specifying a column to be used as row names, or a character or integer vector giving the row names for the data frame.

* stringsAsFactors = logical: should character vectors be converted to factors?
:::
::::

We can select the elements of a dataframe in two ways:

- select a column with the sign `$` and then the element of the column:

```{r}
db$Numbers[c(4,8)]
```

- select the columns and the rows using directly the square brackets `[]`:

```{r}
db[c(4,8), 1]
```
We can see the column names using the `colnames` command:

```{r}
colnames(db)
```
and change them:

```{r}
colnames(db) <- c("Col1", "Col2")
```

The same for the rows using the command `rownames` command.

## List

Lists are one of the most versatile and useful objects in `R`. The list can contain different types of objects as the dataframe but also having different dimensions since the elements of a list are completely indipendent.

We create a list with the `list()` command:

```{r}
my_first_list <- list(my_array = X,
                      my_data_frame = db,
                      my_vector = x)
```

and we can call an object of this list 

- using `$` 

```{r}
my_first_list$my_data_frame[1,2]
```


- using `[[]]` 

```{r}
my_first_list[[2]][1,2]
```
Since we have extracted an object from the list, all the things we have seen for vectors, matrices, dataframes etc can be applied directly.


## Concluding 

We can check the structure of our object (vector, matrix, array, dataframe) and its class using the `str()` function:

```{r}
str(X)
```
or check it using the functions `is.vector()`, `is.matrix()`, `is.array()` and `is.data.frame()`:

```{r}
is.matrix(X)
```

The `summary()` command shows us the structure of the object:

```{r}
summary(X)
```


```{r}
summary(db)
```

and the functions `head()`/`tail()` returns the first/last parts of a vector, matrix, table, data frame or function:

```{r}
head(db)
```

```{r}
tail(db)
```

# Other fundamental functions

- Help?

```{r, eval = FALSE}
help("+") #Looking for help in understanding the use of +
```

- Where I am and where I want to go

```{r, eval = FALSE}
setwd("my_path") #Set Working Directory
getwd() #Get Working Directory
```

- See what is in my working directory?

```{r}
ls() #return a vector of character strings giving the names of the objects in the specified environment.
```

- Save what we found in the working directory or a particular object?

```{r, eval = FALSE}
save(list = ls(), file = "myfirstR.RData") #Save everything in myfirstR.RData
save(x, file = "myfirstR.RData")  #save x in myfirstR.RData
```

- Delete what we found in the working directory or a particular object?

```{r, eval = FALSE}
rm(list = ls()) #Deltere everything in my working directory
rm(x) #Delete x
```

# Load data

How to upload data? Depends... 

First see the format of your document (`.xlsx`, `.csv`, `.xls`, `.txt`, `.RData` etc), in general, there will be a corresponding command:

- `load()`: Reads a `.RData` file.

- `read.table()`: Reads a delimited (space, tab, comma) data file.

- `read.csv()`: Reads a `.csv` file.

- `read_excel()`: Reads a `.xlsx` file. You must install the package `readxl`.

Inside the parenthesis you must specify the path where the file is and some other options $\rightarrow$ check these options using the help function.

For example:

```{r, eval = FALSE}
help("read.table")
```

Alternatively you can use the toolbar:

`Files > Import Datasets >`

# Basic Tidyverse Concepts

The `tidyverse` is not really a package, but a collection of packages. Let install it and load it in our working directory:

```{r, eval = FALSE}
install.packages("tidyverse")
```

```{r}
library(tidyverse)
```

`Tidyverse` is based on the **pipe operator** i.e.,`%>%`. It connects two function calls by making the value returned by the first call the first argument of the second call:

```{r}
c(1:3) %>% rep(3) 
```
returns the same output as

```{r}
rep(c(1:3), 3)
```

We can use the pipe more than one times:

```{r}
c(1:3) %>% rep(3) %>% length()
```

By default the value of the left-hand call is piped into the right-hand call as the first argument. You can make it some other argument by referring to it as the dot `.`, for example:

```{r}
3 %>% rep(c(1:3), .) 
```

## `dplyr`

Inside the `tidyverse` function we have the useful `dplyr` package. We will see only the main functions of this package, but you can use the basic `R` command proposed before instead of these ones. 

- `filter()`: Subset **rows** using column values

- `select()`: Subset **columns** using their names and types

- `mutate()`: Create, modify, and delete **columns**

- `group_by()`: **Group** by one or more variables

- `summarize()`: Summarise each group to **fewer rows**

Let see some examples. First of all, we load the `R` dataset `ChickWeight` from the package `datasets`


```{r}
library(datasets)
data("ChickWeight")
```

You can see the description of this dataset typing `help(ChickWeight)`. So, we want:

- select only the weight and chick columns:

```{r}
ChickWeight %>% select(weight, Chick) %>% head()
```

- and compute the mean of the weight for each chick:

```{r}
ChickWeight %>% select(weight, Chick) %>%
  group_by(Chick) %>%
  summarize(weight = mean(weight, na.omit = TRUE))
```

- filter only the chick with weight greater than the global mean

```{r}
ChickWeight %>% select(weight, Chick) %>%
  group_by(Chick) %>%
  summarize(weight = mean(weight, na.omit = TRUE)) %>%
  filter(weight >= mean(weight))
```

> Why did we look at the `dplyr` package if we could directly use the basic functions in `R`? We will use another useful and famous `R` package which follows a similar structure $\rightarrow$ `ggplot2`!




# A bit of Exploratory Data Analysis (EDA)

This is a crucial part and usually takes up most of the time. A proper and extensive exploratory data analysis (EDA) would reveal interesting patterns and help to prepare the data in a better way for the following analyses. It can be roughly summarised in 3 big parts:

1. _Structure and summary of data_ -- To check the type of variables in the data set and compute location indeces (e.g., mean, median) of the variables of interest.
2. _Exploratory plots_ -- Histograms, boxplots, barplots, correlogram or scatterplots (e.g., skeweness, outliers).
3. _"Cleaning" process_ -- Are there any NAs? Missing values? Integer variables to be converted in factors?

Let's take a real example, we will use the `Happiness and Alcohol Consumpation` data set available at [Kaggle](https://www.kaggle.com/marcospessotto/happiness-and-alcohol-consumption) or in the `Datasets` folder. 

```{r read_example, echo = T}
DB <- read.csv("Datasets/HappinessAlcoholConsumption.csv")
```

The dataset comprises `r library(tidyverse); DB %>% ncol()` different characteristics on `r library(tidyverse); DB %>% nrow()` countries during the year $2016$. In particular, these features were:

- *Country* - Name of the country;
- *Region* - Region the country belongs to;
- *Hemisphere* - Hemisphere of country;
- *HappinessScore* - A metric measured that asks the sampled people the question: "How would you rate your happiness on a scale of $0$ to $10$ where $10$ is the happiest";
- *HDI* - Human Development Index by United Nations Development Programme;
- *GDP_PerCapita* - Gross Domestic Product index;
- *Beer_PerCapita* - Liters ( per capita ) of beer consumption;
- *Spirit_PerCapita* - Liters ( per capita ) of spirit consumption;
- *Wine_PerCapita* - Liters ( per capita ) of wine consumption.

## Structure and summary of data

and see some examples of type of objects:

:::: {style="display: grid; grid-template-columns: auto auto; grid-column-gap: 5px; place-items: start;"}
::: {}
__R commands__ 

```{r check_db, eval = FALSE}
is.data.frame(DB) 
str(DB) 
head(DB) 
View(DB) 
```
:::
::: {}
__Comments__ 

* `is.data.frame(x)` -- check if `x` is a data.frame object.
* `str(x)` -- function display the internal structure of the data.frame object `x`.
* `head(x)` -- return the first six observations (rows) of the data.frame object `x`.
* `View(x)` -- open the data.frame object `x` in another window.
:::
::::

Thanks to the function `str()` we can see the structure of the data, e.g.,:

 1. Numeric : `HappinessScore`, `GDI_PerCapita`;
 
 2. Character: `Country`, `Region`, `Hemisphere`; 
 
 3. Integer: `HDI`, `Beer_PerCapita`, `Spirit_PerCapita`, `Wine_PerCapita` ;  
 
We must transform the variables `Country`, `Region` and `Hemisphere` as factors to be interpreted as categorical variables with `r levels(DB$Country)` levels for `Country`, `r levels(DB$Region)` levels for `Region`, and `r levels(DB$Hemisphere)` levels for `Hemisphere`. 
 
:::: {style="display: grid; grid-template-columns: auto auto; grid-column-gap: 5px; place-items: start;"}
::: {}
__R commands__ 

```{r preprocessing, dat_structure}
# Variable "Country"
DB$Country <- as.factor(DB$Country) 
levels(DB$Country)
# Variable "Region"
DB$Region <- as.factor(DB$Region) 
levels(DB$Region)
# Variable "Hemisphere"
table(DB$Hemisphere)
DB$Hemisphere <- ifelse(DB$Hemisphere == "noth", "north", DB$Hemisphere)
DB$Hemisphere <- as.factor(DB$Hemisphere) 
levels(DB$Hemisphere)
```
:::
::: {}
__Comments__ 

* `as.factor(x)` -- function transforming variable `x` in a categorical one.
* `levels(x)` -- function listing all levels of variable `x`.
* `table(x)` -- check how many values the variable `x` takes.
* `ifelse(cond, x, y)` -- if `cond` is satisfied then return `x` else `y`.
:::
::::

Using `summary()` on an object of class `data.frame`, we can compute basic statistics for each variable in the data set: minimum, maximum and mean values and quartiles:

```{r get_summary}
summary(DB)
```

or we can use the commands `max()`, `min()`, `median()`, `quantile()`, `mean()`.

```{r data_summary}
out <- data.frame(
min = min(DB$HappinessScore),
quantile025 = quantile(DB$HappinessScore, 0.25),
median = median(DB$HappinessScore),
mean = mean(DB$HappinessScore),
quantile075 = quantile(DB$HappinessScore, 0.75),
max = max(DB$HappinessScore))
rownames(out) <- NULL #remove rownames of the objects out
out
```

## Exploratory plots

<br>![](Images/pie1.gif){width=100%}

We make histograms plot for each quantitative variable. We can use base `R` functions:

```{r histograms, fig.height=3}
var_interest_numeric <- colnames(DB)[!(colnames(DB) %in% c("Country", "Region", "Hemisphere"))]

for(i in c(1:length(var_interest_numeric))){
  
  hist(DB[,var_interest_numeric[i]], 
       main = var_interest_numeric[i], 
       xlab = var_interest_numeric[i])
}
```


or we can use the packages `dplyr`, `tidyr`, `ggplot2`:

```{r load_libraries}
library(dplyr)
library(ggplot2)
library(tidyr)
```


```{r example_dplyr, fig.height=3}
DB %>%
  dplyr::select(all_of(var_interest_numeric)) %>%
  gather(cols, value) %>%
  ggplot(aes(x = value)) + geom_histogram(bins = 50) + facet_wrap(.~ cols, ncol = 3, scales = "free")
```

for the factor variables (`Region`, `Hemisphere`), we can use the `barplot` function:

```{r histograms1, fig.height=3}
var_interest_factor <- c("Region", "Hemisphere")
for(i in c(1:length(var_interest_factor))){
  
  barplot(table(DB[,var_interest_factor[i]]), main = var_interest_factor[i], xlab = var_interest_factor[i])
}
```

We do not consider the variable `Region` since it has $122$ unique values. In the same way as before, but using the command `geom_bar()`, we can use the packages `dplyr`, `tidyr`, `ggplot`:

```{r ggplot_example, fig.height=3}
DB %>%
  dplyr::select(all_of(var_interest_factor)) %>%
  gather(cols, value) %>%
  ggplot(aes(x = value)) + geom_bar() + facet_wrap(.~ cols, ncol = 3, scales = "free")
```

In the same way we can produce some boxplot using the basic `R` command `boxplot()` 

```{r histograms2, fig.height=3}
for(i in c(1:length(var_interest_numeric))){
  
  boxplot(DB[,var_interest_numeric[i]], main = var_interest_numeric[i], xlab = var_interest_numeric[i])
}
```

or the `geom_boxplot()` function from the package `ggplot2`:

```{r boxplot, fig.height=3}
DB %>%
  dplyr::select(all_of(var_interest_numeric)) %>%
  gather(cols, value) %>%
  ggplot(aes(x = value)) + geom_boxplot() + facet_wrap(.~ cols, ncol = 3, scales = "free")
```

> **_Question:_** _Are there any missing values or NAs?_

We can check the correlation between the *quantitative* variables to look at the level of linear dependence between pairs of two variables. We can either 

(a) Read the correlation matrix (since the correlation matrix is symmetric -- $COR(X,Y) = COR(Y,X)$ --, we can simply look at the upper or lower triangular version of it).

:::: {style="display: grid; grid-template-columns: 1fr 2fr; grid-column-gap: 20px; place-items: start;"}
::: {}
__R commands__ 

```{r correlation1}
# Correlation matrix
corr_matrix <- round(cor(DB[,var_interest_numeric]), 2)
corr_matrix[lower.tri(corr_matrix)] <- 0
corr_matrix
```
:::
:::{}
__Comments__ 

* First line:
  * `DB[,var_interest_numeric]` -- keep only the columns having name included in the `var_interest_numeric` vector.
  * `cor(DB[...])` -- compute the correlation matrix for all possible couples of variables in the data set.
  * `round(cor(...), 2)` -- rounds to two decimal places all correlation values.
* Second line: keep only the values in the upper triangular part of the matrix for a better reading.
:::
::::

(b) Plot the correlation matrix through a correlogram (higher absolute values of correlation between pairs of variables correspond to more vibrant colors on the cells of the correlogram for those pairs).

:::: {style="display: grid; grid-template-columns: 1fr 2fr; grid-column-gap: 20px; place-items: start;"}
::: {}
__R commands__ 

```{r correlation2, fig.width=14, fig.height=14}
# Correlogram
library(ggcorrplot)                                             
ggcorrplot(corr_matrix, 
           type = "upper", 
           lab = T, 
           lab_size = 7, 
           outline.col = "white", 
           colors = c("tomato2", 
                      "white", 
                      "springgreen3"), 
           title = "", 
           ggtheme = theme_gray, 
           p.mat = cor_pmat(corr_matrix), 
           pch.cex = 30, 
           tl.cex = 20)
```
:::
:::{}
__Comments__ 

* Install and load the library `ggcorrplot` to plot the correlogram. 
* The function to be used is `ggcorrplot(x)`, where `x` is the correlation matrix (basic usage).
* In particular:
  * `corr_matrix` -- the first argument to be passed is the correlation matrix.
  * `type = "upper"` -- print only the upper triangular part of the matrix.
  * `lab = T` -- add correlation values on the plot. 
  * `lab_size = 7` -- dimension of correlation values on the plot.
  * `outline.col = "white"` -- border color of the cells of the correlation values.
  * `colors = c("tomato2", "white", "springgreen3")` -- vector of 3 colors for negative, mid and positive correlation values.
  * `title = ""` -- no title of the plot.
  * `ggtheme = theme_gray` -- ggplot2 function or theme object
  * `p.mat = cor_pmat(corr_matrix)` -- matrix of p-values, computed using the function `cor_pmat()`, from the hypothesis testing procedure with $H_0: correlation = 0$ vs $H_1: correlation \neq 0$. All those correlation values for which the p-value of the test was bigger than significance level $\alpha = 0.05$ are crossed.
  * `pch.cex = 30` -- size of symbol for not statistically significant correlation values.
  * `tl.cex = 20` -- size of variable name labels.
:::
::::

*General note:* If you use a function inside a package only one time, and you don't want to occupy memory when loading all the functions inside that package, once you have installed the latter you can simply call the function of interest by `package::function(...)`. For example, if we only want to use the `ggcorrplot` function from the homonymous package, we can simply use `ggcorrplot::ggcorrplot(x)`.

However, to speed up the code's writing we can use the following function from the `stats` package which is a basic package in `R`:

```{r correlation3, fig.width=14, fig.height=14}
heatmap(corr_matrix)
```

or the `corrplot` function from the `corrplot` package:

```{r correlation4, fig.width=14, fig.height=14}
corrplot::corrplot(corr_matrix)
```


Another way to analyse the relationships between numerical variables is through the graphical explotation of the scatterplots. 

```{r scatterplot_matrices, fig.height=14, fig.width=14}
# We plot a matrix of scatterplot using the "splom" function from the "lattice" library
lattice::splom(x = DB[, var_interest_numeric], pch = "*")
# We can imitate the latter plot using also basic R:
pairs(DB[, var_interest_numeric], pch = "*", col = "#0080ff")
```

> **_Question:_** _What type or relationships can we detect?_

## "Cleaning" process

We can also prepare training and test data sets for prediction for (eventual) future analyses:

```{r train_test}
# Number of total rows in the data set (= maximum sample size from which we can draw random units):
n <- nrow(DB)
# Size of the training data set (3/4 of "census_tracts"):
size <- round(0.75 * n)
# We set a seed for reproducibility of our results:
set.seed(666)
# We sample the row indeces representing the statistical units that will end up in the training set:
row.ind <- sample(x = 1:n, size = size) 
train_db <- DB[row.ind,]
# The remaining row indeces corresponds to the statistical units that end up in the test set:
test_db <- DB[-row.ind,]
# Check on factors (we must have all levels of categorical variables in both training and test sets):
all.equal(levels(train_db$Country), levels(test_db$Country))
all.equal(levels(train_db$Region), levels(test_db$Region))
```

_A little comment:_ When we split the original data set in training and test and there are categorical variables, it is better to check that all levels/categories of those variables are present in both training and test data. 

> **_Question:_** _What could happen if categorical variable $X$ has all its levels in the training set but not in the test set? And if it has all its levels in the test set, but not the training one?_

# Linear regression

<center>
![](Images/regression.png){ width=70% }
</center> 
<br>

## The model

We will start by fitting a simple linear regression model, with `HappinessScore` as the response variable and `Beer_PerCapita` as the predictor: 
\[
  \text{EQ. 1:} \qquad \text{HappinessScore}_i \approx \beta_0 + \beta_1 \, \text{Beer_PerCapita}_i, \qquad i = 1, \dots, n.
\]
The interest lies in testing the coefficients of the linear model, specifically $\beta_1$:

- The null hypothesis is that the coefficient associated with `Beer_PerCapita` is zero -- $H_0: \beta_1 = 0$.
- The alternative hypothesis is that the coefficient associated with `Beer_PerCapita` is not zero -- $H_1: \beta_1 \neq 0$ $\longrightarrow$ there exists a relationship between (the dependent variable) `HappinessScore` and (the independent variable) `HappinessScore`.
- If the $p$-value associated to the estimate of $\beta_1$ is less than 0.05, we usually reject the null hypothesis.

To see detailed results of the model, we can use the `summary` method for class `lm()`, the class of objects representing linear models in `R` (one of them): this gives us $p$-values and standard errors for all coefficients in the model, as well as the $R^2$ statistic and $F$-statistic: the former measures the proportion of the variation in the dependent variable explained by all of independent variables in the model, while, if the latter is statistically significant, the model can be considered "good". 

:::: {style="display: grid; grid-template-columns: auto 2fr; grid-column-gap: 20px; place-items: start;"}
::: {}
__R commands__

```{r linear_regression}
# Simple linear regression model:
lm.fit.simple <- lm(formula = HappinessScore ~ Beer_PerCapita, 
                    # formula equivalent to the model in "EQ. 1"
                    data = DB)
summary(lm.fit.simple)
```
:::
::: {}
__Comments on `summary(lm.fit.simple)` output__

* `Call` section -- print the code of the linear regression model.
* `Residuals` section -- Location indeces of the residuals of the model, computed as $$e_i = y_i - \hat{y}_i = \text{HappinessScore}_i - \widehat{\text{Beer_PerCapita}}_i,$$ for $i = 1, \dots, n$.
* `Coefficients` section -- Matrix of dimension "number of coefficients" $\times$ 4, where the 4 columns are 
  * `Estimate` -- estimates of $\beta_0$ and $\beta_1$, obtained through OLS (ordinary least squares). Let us use $\hat{\beta}_0$ and $\hat{\beta}_1$ as symbols for estimates of $\beta_0$ and $\beta_1$, respectively.
  * `Std. Error` -- standard errors for the estimates of $\beta_0$ and $\beta_1$.
  * `t value` -- t-statistic value corresponding to the tests $$H_0: \beta_0 = 0 \text{ vs } H_1: \beta_0 \neq 0$$ and $$H_0: \beta_1 = 0 \text{ vs } H_1: \beta_1 \neq 0.$$
  * `Pr(>|t|)` -- t-statistics related (two-sided) p-values.
* Other quantities, as $R^2$ and $F$-statistics.
:::
::::

> **_Question:_** _Is the coefficient associated to_ `Beer_PerCapita` _statistically significant? For which significance level $\alpha$? And what can we say about the model we just fitted? Is it good?_

## Interpretation of the results 

The fitted regression line can be written as 
\[ 
\widehat{\text{HappinessScore}}_i = \hat{\beta}_0 + \hat{\beta}_1 \, \text{Beer_PerCapita}_i = 4.78 + 0.005 \, \text{Beer_PerCapita}_i, \qquad i = 1, \dots, n. 
\]

We can say that the coefficient associated to `Beer_PerCapita` is statistically significant at a level $\alpha = 0.05$ (actually, at lower level), meaning that there exists a (strong) relationship between `HappinessScore` and `Beer_PerCapita`. Moreover, since it has a negative sign, we can interpret it as follows: if `Beer_PerCapita` differed by 1\%, `HappinessScore` will increase by `r abs(round(coef(lm.fit.simple)[[2]], 3))` units, on average.

We can compute the $95\%$ confidence interval for the coefficient $\beta_0$ and $\beta_1$:

```{r linear_regression_output}
# Confidence intervals
confint(lm.fit.simple)
```

## Prediction and confidence intervals

The `predict()` function can be used to produce confidence intervals and prediction intervals for the prediction of `HappinessScore` for a given value of `Beer_PerCapita`: the result is a matrix, with number of rows equal to the number of new values of `Beer_PerCapita` we pass to `predict`, and 3 columns, containing in the column 

* `fit` -- the estimated/predicted value of `HappinessScore` for a given value of `Beer_PerCapita`.
* `lwr` -- the lower extreme of the confidence/prediction intervals containing `HappinessScore` for a given value of `Beer_PerCapita`.
* `upr` -- the upper extreme of the confidence/prediction intervals containing `HappinessScore` for a given value of `Beer_PerCapita`.


* A 95\% confidence interval for the __coefficients of the simple linear regression__ is computed as 
\[ 
  \hat{\beta}_0 \pm 1.96 \cdot SE(\hat{\beta}_0) 
\] 
for $\beta_0$, or 
\[ 
  \hat{\beta}_1 \pm 1.96 \cdot SE(\hat{\beta}_1)
\] 
for $\beta_1$.
* A 95\% confidence interval for the __average response__ of the model regards $E(\text{HappinessScore})$.
* A 95\% prediction interval for the __individual response__ of the model regards $\text{HappinessScore}_{\text{new data}}$.

As an example:

```{r linear_regression_prediction}
# Confidence intervals
predict(lm.fit.simple,                                       # lm fitted object
        newdata = data.frame(Beer_PerCapita = (c(40, 125, 200))),          # new values for Beer_PerCapita (to be given as data.frame)
        interval = "confidence")                             # type of interval
# Prediction intervals
predict(lm.fit.simple,                                       # lm fitted object
        newdata = data.frame(Beer_PerCapita = (c(40, 125, 200))),          # new values for Beer_PerCapita (to be given as data.frame)
        interval = "prediction")                             # type of interval
```

As expected, the confidence and prediction intervals are centered around the
same point but the latter are substantially wider.

## Graphical representation of the model

The plot of the regression model with both confidence and prediction intervals can be obtained as follows:

```{r linear_regression_prediction1}
# New data
new_Beer_PerCapita <- seq(min(DB$Beer_PerCapita), max(DB$Beer_PerCapita), by = 0.05)
# 95% confidence interval for regression line
conf_interval <- predict(lm.fit.simple, 
                         newdata = data.frame(Beer_PerCapita = new_Beer_PerCapita), 
                         interval = "confidence", level = 0.95)
# 95% prediction interval for regression line
pred_interval <- predict(lm.fit.simple, 
                         newdata = data.frame(Beer_PerCapita = new_Beer_PerCapita), 
                         interval = "prediction", level = 0.95)

# Plot
plot(x = DB$Beer_PerCapita,                                         # `Beer_PerCapita` on x-axis
     y = DB$HappinessScore,                                          # `HappinessScore` on y-axis
     pch = "*", cex = 0.8,                                            # graphical parameter
     xlab = "Beer Per Capita",                  # labels of x-axis and y-axis
     ylab = "mHappiness Score")
abline(lm.fit.simple, col = "blue")                          # regression line
lines(new_Beer_PerCapita, conf_interval[,2], col = "blue", lty = 2)            # lower extreme conf. interval
lines(new_Beer_PerCapita, conf_interval[,3], col = "blue", lty = 2)            # upper extreme conf. interval
lines(new_Beer_PerCapita, pred_interval[,2], col = "red", lty = 2)             # lower extreme pred. interval
lines(new_Beer_PerCapita, pred_interval[,3], col = "red", lty = 2)             # upper extreme pred. interval
legend("topright",                                                    # legend
       title = expression(1-alpha == 0.95 ~ ("95%")),
       legend = c("Conf. interval", "Pred. interval"),
       lty = 2, col = c("blue", "red"))
```

## Diagnostics

There is some evidence for non-linearity in the relationship between `Beer_PerCapita` and `HappinessScore`.

This non-linearity can be examineed also through some diagnostic plots. In order:

* **Top-left plot** -- **Residual versus fitted values** <br> This plot shows if residuals have non-linear behaviour, which may happen in case of a non-linear relationship between the predictor and the response. If you find equally spread residuals around a horizontal line in 0 without distinct patterns, that is a good indication you do not have non-linear relationships. 
* **Top-right plot** -- **Normal Q-Q plot** <br> This plot shows if residuals are normally distributed. It is a scatterplot created by plotting the theoretical quantiles of a standard normal distribution against the quantiles of the standardized residuals: if both sets of quantiles came from the same distribution (i.e., standard normal), we should see the points forming a line that is roughly straight. 
* **Bottom-left plot** -- **Scale-Location comparison** <br> This plot shows if residuals are spread equally along the ranges of predictors. This is how you can check the assumption of homoscedasticity in your data (homoscedasticity means equal variance). If you see a horizontal line in 1 with equally (randomly) spread points, than the situation is good.
* **Bottom-right plot** -- **Residuals vs Leverage** <br> This plot helps us to find influential cases (i.e., subjects) if any. Not all outliers are influential in linear regression analysis: even though data have extreme values, the regression results wouldn't be much different if we either include or exclude them from analysis. On the other hand, some cases could be very influential even if they look to be within a reasonable range of the values. To detect those values, we observe if there are values at the upper/lower right corner of this graph, outside of a dashed line called Cook's distance: when this happens, the cases are influential to the regression results, meaning that the regression results will be altered if we exclude them.

:::: {style="display: grid; grid-template-columns: auto auto; grid-column-gap: 20px; place-items: start;"}
::: {}
__R commands__

```{r linear_regression_diagnostic, fig.height=10, fig.width=10, echo=c(1,2,3,5,6)}
# Diagnostic plots
par(mfrow = c(2,2))
plot(lm.fit.simple)
par(mfrow = c(1,1))
# # Alternatively
# autoplot(lm.fit.simple, label.size = 3)
```
:::
::: {}
__Comments__

* **Residual versus fitted values** -- We can see a parabolic pattern in our case, meaning that there is a non-linear relationship that was not explained by the model and was left out in the residuals.
* **Normal Q-Q plot** -- The points do not follow the straight line, meaning that the standardized residuals probably not follow a standard normal distribution (i.e., the residuals are not normal).
* **Scale-Location** -- The residuals seem to follow the 0 value along the x-axis, but probably there is some influential values since the red line is not perfectly straight.
* **Residuals vs Leverage** -- This is a typical case when there are no influential observations. The Cookâ€™s distance lines (i.e., red dashed lines) are not seen because all cases are well inside of it. 

:::
::::

## Multiple linear regression

We can add more variables to the model, keeping in mind that now we have both quantitative and qualitative variables to take into consideration as predictors. 
\[
 \text{EQ. 2:} \qquad  \text{HappinessScore}_i \approx \beta_0 + \beta_1 \, \text{Beer_PerCapita}_i + \beta_2 \text{HDI}_i + \dots + \beta_{3} \text{Wine_PerCapita}_i, \qquad i = 1, \dots, n.
\]
Below we use the syntax `formula = response variable ~.` inside `lm()` to include all the variables except `HappinessScore` in the model as linear independent predictors, while `-Country` after the symbol `~` permits to not consider the `Country` variable into the model.

> **_Question:_** _What does we find? How can we interpret the results of the regression models? And in terms of goodness of the model, what does the $F$-statistic tell us?_

```{r multiple_linear_regression, echo=1:2}
lm.fit.multiple <- lm(HappinessScore ~. -Country, data = DB)
summary(lm.fit.multiple)
# car::vif(lm.fit.multiple)
```

## Interaction terms

It is easy to include interaction terms in a linear model using the `lm()` function.

```{r multiple_linear_regression_interactions}
lm.fit.interactions <- lm(HappinessScore ~ Beer_PerCapita*Hemisphere, data = DB)
summary(lm.fit.interactions)
```

> **_Question:_** _What does we find? How can we interpret the results of the regression models? And in terms of goodness of the model, what does the $F$-statistic tell us?_

## Non-linear transformations of the predictors

The `lm()` function can also accommodate non-linear transformations of the predictors. For instance:

```{r multiple_linear_regression_nonlinear}
lm.fit.nonlinear <- lm(HappinessScore ~ Beer_PerCapita + I(Beer_PerCapita^2), data = DB)
summary(lm.fit.nonlinear)
```

The near-zero $p$-value associated with the quadratic term suggests that it leads to an improved model. We use the `anova()` function to further quantify the extent to which the quadratic fit is superior to the linear fit:

```{r multiple_linear_regression_anova}
anova(lm.fit.simple, lm.fit.nonlinear)
```

The `anova()` function performs a hypothesis test comparing the two models. The null hypothesis is that the two models fit the data equally well, and the alternative hypothesis is that the model with both linear and quadratic terms is superior. Here the $F$-statistic is 14.7 and the associated $p$-value is near zero. This provides evidence that the model containing the predictors `Beer_PerCapita` and `Beer_PerCapita`$^2$ is far superior to the model that only contains the predictor `Beer_PerCapita`.

> **_Question:_** _Is this surprising or not? And what can we say from the diagnostics below?_

```{r multiple_linear_regression_diagnostic, fig.height=10, fig.width=10, echo=c(1,2,4,5)}
par(mfrow = c(2,2))
plot(lm.fit.nonlinear)
par(mfrow = c(1,1))
# # Alternatively
# autoplot(lm.fit.nonlinear, label.size = 3)
```


# Concluding.. Do you want to become a supeR useR?

What to do?

- Train `r emo::ji("sports")`

- Work out the bugs yourself `r emo::ji("bug")`

- Document yourself `r emo::ji("book")`:

   - Google, [R-bloggers](https://www.r-bloggers.com/), ...

   - [R cookbook](https://rc2e.com/)
   
   - [R in a Nutshell](https://www.amazon.com/R-Nutshell-In-OReilly/dp/144931208X/ref=dp_rm_title_3)
   
   - [Learning R](https://www.amazon.com/Learning-R-Richard-Cotton/dp/1449357105/ref=dp_rm_title_1)
   
   - [An Introduction to Statistical Learning: with Applications in R](https://hastie.su.domains/ISLR2/ISLRv2_website.pdf)
   
   - [The Tidyverse cookbook](https://rstudio-education.github.io/tidyverse-cookbook/)

   
  

